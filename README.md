

WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and Other Language Editions

This repository contains the Chrome extension and companion resources described in our paper:

WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and Other Language Editions

⸻

🔍 Overview

WikiGap is a browser extension that reveals factual asymmetries between English Wikipedia and selected non-English editions (Chinese, French, Russian). It helps to:
	•	Challenge the “English-as-superset” assumption
	•	Increase cross-lingual awareness and access
	•	Support multilingual knowledge equity

All fact-extraction and comparison logic lives in the separate InfoGap pipeline **(WikiGap branch)**:

https://github.com/smfsamir/infogap

This repo focuses exclusively on the UI layer—loading the pre-computed JSON files and presenting knowledge gaps in the page.


🗂️ Repository Structure

Path / File	Purpose
content.js	Core script injected into Wikipedia pages
manifest.json	Chrome extension manifest
wikigap/process_annotations.py	Converts InfoGap output into the concise topic.json format consumed by the extension
quiz_questions/	Sample multiple-choice questions from our user study
studyeval/ (git-ignored)	Notebooks & scripts for analyzing user-study data
data/	Place to drop the JSON files generated by InfoGap (see steps below)


⸻

🚀 Quick Start: Testing the Extension
	1.	Clone the repo

git clone https://github.com/aw814/WikiGap.git


	2.	Zip the contents of the cloned directory (zip the files inside, not the parent folder).
	3.	Load the unpacked extension in Chrome:
	•	Navigate to chrome://extensions/
	•	Toggle Developer mode (top-right)
	•	Click Load unpacked, select the unzipped directory
	4.	Visit one of the sample English-language articles included in the dataset (e.g., Peking Duck, Philippine Adobo, Paella, Injera, Wiener Schnitzel). You should see WikiGap highlight cross-lingual facts in the sidebar.

⸻

Below is an updated “Using WikiGap on Additional Articles” section that spells out the two-stage InfoGap workflow and clarifies exactly which files you must place in this repo (data_pipeline/wikigap_data/json/{topic}.json).
You can replace the old section in README.md with the version below.

⸻

🌐 Using WikiGap on Additional Articles

WikiGap’s UI expects a single, unified file per article (e.g., Peking_Duck.json). Producing that file is a two-step process inside the InfoGap repo:

Stage	Where it runs	Output file(s)	Purpose
1 A. Run the “complete” InfoGap pipeline	infogap/	annotation_<DATE>_<ARTICLE_TOPIC>_<LANG>.json (one per language)	Raw fact-mismatch annotations generated by GPT
1 B. Post-process those annotation files (InfoGap’s dedicated script)	infogap/	{ARTICLE_TOPIC}.json	Combines multi-language outputs into a single, easy-to-retrieve file for WikiGap

✨ Note : Step 1 B is novel to the WikiGap project—it standardises the raw annotations for real-time look-ups inside the extension.

Step 1 Generate {topic}.json via InfoGap
	1.	Run the main pipeline (InfoGap README → run-multiple-topics, etc.).
This produces the per-language annotation_<…>.json files.
	2.	Run the post-processing script provided in InfoGap (e.g.,

python postprocess/build_topic_json.py

or equivalent).
It collapses all annotation_*.json for a given article into one {topic}.json.

Step 2 Copy Files Into This Repo

Place each final {topic}.json under:

WikiGap/
   └─ json/
      └─ {topic}.json     ← (one per article)

Step 3 Refresh the Extension

Reload WikiGap in chrome://extensions/, then visit the corresponding English Wikipedia article—your new cross-lingual facts will surface automatically.



📄 Paper

Preprint: https://arxiv.org/abs/2505.24195

⸻

🤝 Citation

If you use this codebase or the accompanying datasets, please cite the paper above.
# WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and Other Language Editions

This repository contains the **Chrome extension** and companion resources described in our paper:

> **WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and Other Language Editions**

---

## 🔍 Overview

WikiGap is a browser extension that reveals factual asymmetries between English Wikipedia and selected non‑English editions (Chinese, French, Russian). It helps to:

- Challenge the “English‑as‑superset” assumption  
- Increase cross‑lingual awareness and access  
- Support multilingual knowledge equity  

All fact‑extraction and comparison logic lives in the separate **InfoGap pipeline (WikiGap branch)**: <https://github.com/smfsamir/infogap>

This repo focuses exclusively on the **UI layer**—loading the pre‑computed JSON files and presenting knowledge gaps directly in the page.

---

## 🗂️ Repository Structure

| Path / File                              | Purpose                                                                                                     |
|------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| `content.js`                             | Core script injected into Wikipedia pages                                                                   |
| `manifest.json`                          | Chrome extension manifest                                                                                   |
| `wikigap/process_annotations.py`         | Converts InfoGap output into the concise `topic.json` format consumed by the extension                      |
| `quiz_questions/`                        | Sample multiple‑choice questions from our user study                                                        |
| `studyeval/` (git‑ignored)               | Notebooks & scripts for analyzing user‑study data                                                           |
| `data_pipeline/wikigap_data/json/`       | **Drop the final `{topic}.json` files** generated by InfoGap here                                           |

---

## 🚀 Quick Start: Testing the Extension

1. **Clone** the repo  
   ```bash
   git clone https://github.com/aw814/WikiGap.git
   ```
2. **Zip** the *contents* of the cloned directory (zip the files inside, **not** the parent folder).  
3. **Load** the unpacked extension in Chrome:  
   - Navigate to `chrome://extensions/`  
   - Toggle **Developer mode** (top‑right)  
   - Click **Load unpacked** and select the unzipped directory  
4. Visit one of the sample English‑language articles included in the dataset (e.g., *Peking Duck*, *Philippine Adobo*, *Paella*, *Injera*, *Wiener Schnitzel*). WikiGap should highlight cross‑lingual facts in the sidebar.

---

## 🌐 Using WikiGap on Additional Articles

WikiGap’s UI expects a **single, unified file** per article (e.g., `Peking_Duck.json`). Producing that file is a two‑step process *inside the InfoGap repo*:

| Stage | Where it runs | Output file(s) | Purpose |
|-------|---------------|----------------|---------|
| **1 A.** Run the full InfoGap pipeline | `infogap/` | `annotation_<DATE>_<ARTICLE_TOPIC>_<LANG>.json` (one per language) | Raw fact‑mismatch annotations generated by GPT |
| **1 B.** **Post‑process** those annotation files | `infogap/` | `{ARTICLE_TOPIC}.json` | Combines multi‑language outputs into a single, easy‑to‑retrieve file for WikiGap |

> **Note :** Step 1 B is novel to the WikiGap project—it standardises the raw annotations for real‑time look‑ups inside the extension.

### 1. Generate `{topic}.json` via InfoGap

```bash
# inside the InfoGap repo
python main_complete_analysis.py run-multiple-topics   # produces annotation_*.json
python postprocess/build_topic_json.py                 # collapses into {topic}.json
```

### 2. Copy files into this repo

Place each final `{topic}.json` under:

```
WikiGap/
└─ data_pipeline/
   └─ wikigap_data/
      └─ json/
         └─ {topic}.json
```

### 3. Refresh the extension

Reload WikiGap in `chrome://extensions/`, then visit the corresponding English Wikipedia article—your new cross‑lingual facts will surface automatically.

---

## 📄 Paper

Preprint available on arXiv: <https://arxiv.org/abs/2505.24195>

---

## 🤝 Citation

If you use this codebase or the accompanying datasets, please cite the paper above.